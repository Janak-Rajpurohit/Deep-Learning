{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fer2013.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35882</th>\n",
       "      <td>6</td>\n",
       "      <td>50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35883</th>\n",
       "      <td>3</td>\n",
       "      <td>178 174 172 173 181 188 191 194 196 199 200 20...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35884</th>\n",
       "      <td>0</td>\n",
       "      <td>17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35885</th>\n",
       "      <td>3</td>\n",
       "      <td>30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35886</th>\n",
       "      <td>2</td>\n",
       "      <td>19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35887 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       emotion                                             pixels        Usage\n",
       "0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...     Training\n",
       "1            0  151 150 147 155 148 133 111 140 170 174 182 15...     Training\n",
       "2            2  231 212 156 164 174 138 161 173 182 200 106 38...     Training\n",
       "3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...     Training\n",
       "4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...     Training\n",
       "...        ...                                                ...          ...\n",
       "35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
       "35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
       "35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
       "35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
       "35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest\n",
       "\n",
       "[35887 rows x 3 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(\"fer2013.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ('angry', 'disgust', 'fear',\\\n",
    "                      'happy', 'sad', 'surprise', 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janak\\AppData\\Local\\Temp\\ipykernel_7684\\2667547461.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"pixels\"][i]=l\n"
     ]
    }
   ],
   "source": [
    "# converting pixels in string -> int -> np array -> reshaping 48,48,1\n",
    "for i in range(len(df.pixels)):\n",
    "    l = df[\"pixels\"][i].split(\" \")\n",
    "    l=np.array(list(map(int,l))).reshape(48,48,1)\n",
    "    df[\"pixels\"][i]=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48, 1)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"pixels\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D , MaxPooling2D,\\\n",
    "    Dropout,Flatten,Dense,Activation,BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convollutional Block 1\n",
    "model.add(Conv2D(32,(2,2),activation='relu',input_shape=(48,48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Dropout(0.20))\n",
    "# Convollutional Block 1\n",
    "model.add(Conv2D(64,(2,2),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Dropout(0.20))\n",
    "# Convollutional Block 1\n",
    "model.add(Conv2D(128,(2,2),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Flatten())\n",
    "# 1 Hidden Layer\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.30))\n",
    "# 2 Hidden Layer\n",
    "model.add(Dense(2,activation='relu'))\n",
    "\n",
    "model.compile(\n",
    "    loss=\"SparseCategoricalCrossentropy\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_7 (Conv2D)           (None, 47, 47, 32)        160       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 47, 47, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 47, 47, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 47, 47, 32)        0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 46, 46, 64)        8256      \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 46, 46, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 46, 46, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 46, 46, 64)        0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 45, 45, 128)       32896     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 45, 45, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 45, 45, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 45, 45, 128)       0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 259200)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               132710912 \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 132,756,194\n",
      "Trainable params: 132,754,722\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping , ReduceLROnPlateau\n",
    "earlystopping = EarlyStopping(patience=10)\n",
    "learning_rate_reduction=ReduceLROnPlateau(monitor=\"val_acc\",patience=2,verbose=1,factor=0.5,minn_lr=0.00001)\n",
    "callbacks = [earlystopping,learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 2)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df[[\"emotion\",\"pixels\"]][df[\"Usage\"]==\"Training\"]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28709, 2)\n",
      "(3589,)\n",
      "(3589,)\n",
      "(3589,)\n"
     ]
    }
   ],
   "source": [
    "train_df = df[[\"emotion\",\"pixels\"]][df[\"Usage\"]==\"Training\"]\n",
    "validate_df_x =df[\"pixels\"][df[\"Usage\"]==\"PublicTest\"]\n",
    "validate_df_y = df[\"emotion\"][df[\"Usage\"]==\"PublicTest\"].astype(np.int8)\n",
    "test_df = df[\"pixels\"][df[\"Usage\"]==\"PrivateTest\"]\n",
    "print(train_df.shape)\n",
    "print(validate_df_x.shape)\n",
    "print(validate_df_y.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(train_df[\"pixels\"])\n",
    "y=np.array(train_df[\"emotion\"]).astype(np.int8)\n",
    "y=train_df[\"emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janak\\AppData\\Local\\Temp\\ipykernel_7684\\1919798020.py:6: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  x = np.array(train_df[\"pixels\"].apply(lambda x: np.fromstring(x, sep=' ', dtype=int)))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\PYTHON_PROGRAMS\\DL\\emotion_detection_own.ipynb Cell 13\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(x))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Convert to PyTorch tensor\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# If needed, reshape the tensor to match the expected input shape\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m48\u001b[39m, \u001b[39m48\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# Assuming images are 48x48 and grayscale\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming x is a NumPy array of pixel data\n",
    "\n",
    "x = np.array(train_df[\"pixels\"].apply(lambda x: np.fromstring(x, sep=' ', dtype=int)))\n",
    "print(type(x))\n",
    "# Convert to PyTorch tensor\n",
    "x = torch.from_numpy(x)\n",
    "\n",
    "# If needed, reshape the tensor to match the expected input shape\n",
    "x = x.reshape(-1, 48, 48, 1)  # Assuming images are 48x48 and grayscale\n",
    "\n",
    "# Convert to float and normalize if needed\n",
    "x = x.float() / 255.0  # Normalize to [0, 1] range\n",
    "\n",
    "# Now x is a PyTorch tensor that you can use in your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = (train_df[\"pixels\"])\n",
    "# y = np.array(train_df[\"emotion\"]).astype(np.int8)\n",
    "\n",
    "# Assuming each image is 48x48 pixels and grayscale\n",
    "# x = x.reshape( -1,48, 48, 1)\n",
    "print(type(y[0]))\n",
    "# Convert to TensorFlow tensors\n",
    "# x = tf.convert_to_tensor(x)\n",
    "# y = tf.convert_to_tensor(y)\n",
    "\n",
    "# Now you can use x and y in the model.fit() function\n",
    "# model.fit(x, y, batch_size=30, epochs=30, verbose=1, validation_data=(validate_df_x, validate_df_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\PYTHON_PROGRAMS\\DL\\emotion_detection_own.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x,y,batch_size\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,validation_data\u001b[39m=\u001b[39;49m(validate_df_x,validate_df_y))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "model.fit(x,y,batch_size=30,epochs=30,verbose=1,validation_data=(validate_df_x,validate_df_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions= ('angry', 'disgust', 'fear', \\\n",
    "                     'happy', 'sad', 'surprise', 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_str = pd.read_csv(\"fer2013.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic  = {i :emotions[i] for i in range(len(emotions))}\n",
    "df_str=df_str.replace(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df_str = df_str[[\"emotion\",\"pixels\"]][df_str[\"Usage\"]==\"Training\"]\n",
    "validate_df_str = df_str[\"pixels\"][df_str[\"Usage\"]==\"PublicTest\"]\n",
    "test_df_str = df_str[\"pixels\"][df_str[\"Usage\"]==\"PrivateTest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 validated image filenames belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janak\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\preprocessing\\image.py:989: UserWarning: Found 28709 invalid image filename(s) in x_col=\"pixels\". These filename(s) will be ignored.\n",
      "  warnings.warn('Found {} invalid image filename(s) in x_col=\"{}\". '\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'pixels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3799\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\_libs\\index.pyx:146\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\index_class_helper.pxi:49\u001b[0m, in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pixels'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\PYTHON_PROGRAMS\\DL\\emotion_detection_own.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m train_generator \u001b[39m=\u001b[39m train_datagen\u001b[39m.\u001b[39mflow_from_dataframe(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     train_df_str,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfer2013.csv\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m validate_datagen\u001b[39m=\u001b[39mImageDataGenerator(rescale\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m validate_generator \u001b[39m=\u001b[39m validate_datagen\u001b[39m.\u001b[39;49mflow_from_dataframe(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     validate_df_str,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mfer2013.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     x_col\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpixels\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     y_col\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39memotion\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     target_size\u001b[39m=\u001b[39;49m(\u001b[39m48\u001b[39;49m,\u001b[39m48\u001b[39;49m,\u001b[39m1\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     class_mode\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mvalidate_data generated\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m test_datagen  \u001b[39m=\u001b[39m ImageDataGenerator(rotation_range \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m                                 rescale \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m                                 shear_range\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m                                 height_shift_range\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PYTHON_PROGRAMS/DL/emotion_detection_own.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m                                 )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\preprocessing\\image.py:1610\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_dataframe\u001b[1;34m(self, dataframe, directory, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, subset, interpolation, validate_filenames, **kwargs)\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mdrop_duplicates\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m   1605\u001b[0m   warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1606\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mdrop_duplicates is deprecated, you can drop duplicates \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1607\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mby using the pandas.DataFrame.drop_duplicates method.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1608\u001b[0m       \u001b[39mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m-> 1610\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameIterator(\n\u001b[0;32m   1611\u001b[0m     dataframe,\n\u001b[0;32m   1612\u001b[0m     directory,\n\u001b[0;32m   1613\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1614\u001b[0m     x_col\u001b[39m=\u001b[39;49mx_col,\n\u001b[0;32m   1615\u001b[0m     y_col\u001b[39m=\u001b[39;49my_col,\n\u001b[0;32m   1616\u001b[0m     weight_col\u001b[39m=\u001b[39;49mweight_col,\n\u001b[0;32m   1617\u001b[0m     target_size\u001b[39m=\u001b[39;49mtarget_size,\n\u001b[0;32m   1618\u001b[0m     color_mode\u001b[39m=\u001b[39;49mcolor_mode,\n\u001b[0;32m   1619\u001b[0m     classes\u001b[39m=\u001b[39;49mclasses,\n\u001b[0;32m   1620\u001b[0m     class_mode\u001b[39m=\u001b[39;49mclass_mode,\n\u001b[0;32m   1621\u001b[0m     data_format\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_format,\n\u001b[0;32m   1622\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1623\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1624\u001b[0m     seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m   1625\u001b[0m     save_to_dir\u001b[39m=\u001b[39;49msave_to_dir,\n\u001b[0;32m   1626\u001b[0m     save_prefix\u001b[39m=\u001b[39;49msave_prefix,\n\u001b[0;32m   1627\u001b[0m     save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[0;32m   1628\u001b[0m     subset\u001b[39m=\u001b[39;49msubset,\n\u001b[0;32m   1629\u001b[0m     interpolation\u001b[39m=\u001b[39;49minterpolation,\n\u001b[0;32m   1630\u001b[0m     validate_filenames\u001b[39m=\u001b[39;49mvalidate_filenames,\n\u001b[0;32m   1631\u001b[0m     dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\preprocessing\\image.py:853\u001b[0m, in \u001b[0;36mDataFrameIterator.__init__\u001b[1;34m(self, dataframe, directory, image_data_generator, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, subset, interpolation, keep_aspect_ratio, dtype, validate_filenames)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39m=\u001b[39m dtype\n\u001b[0;32m    852\u001b[0m \u001b[39m# check that inputs match the required class_mode\u001b[39;00m\n\u001b[1;32m--> 853\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_params(df, x_col, y_col, weight_col, classes)\n\u001b[0;32m    854\u001b[0m \u001b[39mif\u001b[39;00m validate_filenames:  \u001b[39m# check which image files are valid and keep them\u001b[39;00m\n\u001b[0;32m    855\u001b[0m   df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filter_valid_filepaths(df, x_col)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\preprocessing\\image.py:901\u001b[0m, in \u001b[0;36mDataFrameIterator._check_params\u001b[1;34m(self, df, x_col, y_col, weight_col, classes)\u001b[0m\n\u001b[0;32m    896\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    897\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mIf class_mode=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, y_col must be a list. Received \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    898\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_mode,\n\u001b[0;32m    899\u001b[0m           \u001b[39mtype\u001b[39m(y_col)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n\u001b[0;32m    900\u001b[0m \u001b[39m# check that filenames/filepaths column values are all strings\u001b[39;00m\n\u001b[1;32m--> 901\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(df[x_col]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39misinstance\u001b[39m(x, \u001b[39mstr\u001b[39m))):\n\u001b[0;32m    902\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    903\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mAll values in column x_col=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m must be strings.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(x_col))\n\u001b[0;32m    904\u001b[0m \u001b[39m# check labels are string if class_mode is binary or sparse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\series.py:982\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m    981\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 982\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m    984\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    985\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    986\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    987\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\series.py:1092\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1089\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1091\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1092\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1093\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pixels'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical,load_img\n",
    "\n",
    "batch_size=15\n",
    "train_datagen  = ImageDataGenerator(rotation_range = 10,\n",
    "                                rescale = 1./255,\n",
    "                                shear_range=0.1,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                width_shift_range=0.1,\n",
    "                                height_shift_range=0.1\n",
    "                                )\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df_str,\n",
    "    \"fer2013.csv\",\n",
    "    x_col=\"pixels\",\n",
    "    y_col = \"emotion\",\n",
    "    class_mode = \"categorical\",\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validate_datagen=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validate_generator = validate_datagen.flow_from_dataframe(\n",
    "    validate_df_str,\n",
    "    \"fer2013.csv\",\n",
    "    x_col=\"pixels\",\n",
    "    y_col=\"emotion\",\n",
    "    target_size=(48,48,1),\n",
    "    class_mode=None,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "print(\"validate_data generated\")\n",
    "\n",
    "test_datagen  = ImageDataGenerator(rotation_range = 10,\n",
    "                                rescale = 1./255,\n",
    "                                shear_range=0.1,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                width_shift_range=0.1,\n",
    "                                height_shift_range=0.1\n",
    "                                )\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df_str,\n",
    "    \"fer2013.csv\",\n",
    "    x_col = \"pixels\",\n",
    "    y_col = \"emotion\",\n",
    "    target_size=(48,48,1),\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
